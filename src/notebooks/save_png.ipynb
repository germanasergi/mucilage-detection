{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5a00a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pycocotools.coco import COCO\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycocotools import mask as maskUtils\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dc7934",
   "metadata": {},
   "source": [
    "Create csv excluding the NaN patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d387f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === CONFIG ===\n",
    "LABELS_FILE = \"/home/ubuntu/mucilage_pipeline/mucilage-detection/csv/patches_final.csv\"\n",
    "OUTPUT_CSV = \"/home/ubuntu/mucilage_pipeline/mucilage-detection/csv/patches_final_filtered.csv\"\n",
    "REMOVED_IDX_FILE = \"/home/ubuntu/mucilage_pipeline/mucilage-detection/csv/removed_indices.npy\"\n",
    "\n",
    "TARGET_RES = \"r10m\"\n",
    "PATCH_SIZE = 256\n",
    "BANDS = [\"b01\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"b08\", \"b8a\", \"b11\", \"b12\"]\n",
    "\n",
    "# Same split function\n",
    "def split_data(labels_file, test_size=0.3, val_size=0.5, seed=42):\n",
    "    df = pd.read_csv(labels_file)\n",
    "    df_train, df_tmp = train_test_split(df, test_size=test_size, stratify=df[\"label\"], random_state=seed)\n",
    "    df_test, df_val = train_test_split(df_tmp, test_size=val_size, stratify=df_tmp[\"label\"], random_state=seed)\n",
    "    return df_train.reset_index(drop=True), df_val.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "\n",
    "def resample_band(ds, band, target_res=\"r10m\", ref=\"b04\", crs=\"EPSG:32632\"):\n",
    "    \"\"\"\n",
    "    Resample any band (reflectance or classification) to target resolution.\n",
    "    \"\"\"\n",
    "    ref_band = ds[f\"measurements/reflectance/{target_res}/{ref}\"].rio.write_crs(crs) # Reference band at target resolution\n",
    "\n",
    "    if band == \"scl\":\n",
    "        band_da = ds[f\"conditions/mask/l2a_classification/r20m/{band}\"].rio.write_crs(crs)\n",
    "        source_res = \"r20m\"\n",
    "    else:\n",
    "        # Detect which reflectance resolution contains the band\n",
    "        source_res = next(\n",
    "        (r for r in [\"r10m\", \"r20m\", \"r60m\"] if band in ds[f\"measurements/reflectance/{r}\"]),\n",
    "        None\n",
    "        )\n",
    "        if source_res is None:\n",
    "            raise ValueError(f\"Band {band} not found in reflectance or scl folder\")\n",
    "        band_da = ds[f\"measurements/reflectance/{source_res}/{band}\"].rio.write_crs(crs)\n",
    "    # If source == target, no resampling needed\n",
    "    if source_res == target_res:\n",
    "        return band_da\n",
    "\n",
    "    return band_da.rio.reproject_match(ref_band)\n",
    "\n",
    "def build_stack(ds, bands, target_res=\"r10m\", ref_band=\"b04\", crs=\"EPSG:32632\"):\n",
    "    \"\"\"\n",
    "    Build a lazy dask-backed (H, W, C) stack from bands, resampling as needed.\n",
    "\n",
    "    Args:\n",
    "        ds: xarray Dataset or DataTree\n",
    "        bands: list of band names to include\n",
    "        target_res: desired output resolution for all bands\n",
    "        ref_band: reference band for resampling (default: 'b04' red)\n",
    "        crs: CRS to assign if missing\n",
    "\n",
    "    Returns:\n",
    "        xarray.DataArray with dimensions (y, x, band)\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "\n",
    "    for b in bands:\n",
    "        if b in ds['measurements/reflectance/r10m'] or \\\n",
    "           b in ds['measurements/reflectance/r20m'] or \\\n",
    "           b in ds['measurements/reflectance/r60m']:\n",
    "            arr = resample_band(ds, b, target_res=target_res, ref=ref_band, crs=crs) / 10000.0\n",
    "        else:\n",
    "            raise ValueError(f\"Band {b} not found or not supported.\")\n",
    "\n",
    "        # Expand dims for stacking\n",
    "        arr = arr.expand_dims(band=[b])\n",
    "        stack.append(arr)\n",
    "\n",
    "    # Concatenate all bands along 'band' dimension\n",
    "    stacked = xr.concat(stack, dim=\"band\").transpose(\"y\", \"x\", \"band\")\n",
    "    return stacked\n",
    "\n",
    "# Load splits\n",
    "df_train, df_val, df_test = split_data(LABELS_FILE)\n",
    "splits = {\"train\": df_train, \"val\": df_val, \"test\": df_test}\n",
    "\n",
    "# Store valid/invalid indices (in original CSV index space)\n",
    "valid_indices_all = []\n",
    "invalid_indices_all = []\n",
    "\n",
    "for split_name, df_split in splits.items():\n",
    "    print(f\"\\nüîç Checking split: {split_name} ({len(df_split)} rows)\")\n",
    "    all_patches = []\n",
    "    all_labels = []\n",
    "    valid_indices = []\n",
    "    invalid_indices = []\n",
    "\n",
    "    for zarr_path, group in tqdm(df_split.groupby(\"zarr_path\"), desc=f\"Scanning {split_name}\"):\n",
    "        if not os.path.exists(zarr_path):\n",
    "            print(f\"‚ö†Ô∏è Missing file: {zarr_path}\")\n",
    "            continue\n",
    "        ds = xr.open_datatree(zarr_path, engine=\"zarr\", mask_and_scale=False, chunks={})\n",
    "        stack = build_stack(ds, BANDS, target_res=TARGET_RES, ref_band=\"b04\")\n",
    "\n",
    "        for idx, row in group.iterrows():\n",
    "            x, y, label = row[\"x\"], row[\"y\"], row[\"label\"]\n",
    "            x_rescaled = x\n",
    "            y_rescaled = y\n",
    "            patch_size_rescaled = PATCH_SIZE\n",
    "            patch = stack.isel(\n",
    "                y=slice(y_rescaled, y_rescaled + patch_size_rescaled),\n",
    "                x=slice(x_rescaled, x_rescaled + patch_size_rescaled)\n",
    "            ).to_numpy().astype(np.float32)\n",
    "\n",
    "            if np.isnan(patch).any() or np.isinf(patch).any():\n",
    "                invalid_indices.append(idx)\n",
    "                continue  # skip invalid patch\n",
    "            \n",
    "            all_patches.append(patch)\n",
    "            all_labels.append(label)\n",
    "            valid_indices.append(idx)\n",
    "        ds.close()\n",
    "    X = np.stack(all_patches, axis=0)   # (N, H, W, C)\n",
    "    y = np.array(all_labels)\n",
    "    np.savez_compressed(f\"/home/ubuntu/mucilage_pipeline/mucilage-detection/saved_npy/{split_name}_cache.npz\", X=X, y=y)\n",
    "    valid_indices_all.extend(valid_indices)\n",
    "    invalid_indices_all.extend(invalid_indices)\n",
    "    print(f\"Found {len(valid_indices)} valid patches in {split_name}\")\n",
    "\n",
    "df_concat = pd.concat([df_train, df_val, df_test])\n",
    "# df_concat still has original indices (because split_data returned them)\n",
    "df_filtered_in_split_order = df_concat.loc[df_concat.index.isin(valid_indices_all)].copy()\n",
    "\n",
    "# Reset index for neat CSV (but the patch order in file is now train->val->test)\n",
    "df_filtered_in_split_order = df_filtered_in_split_order.reset_index(drop=True)\n",
    "df_filtered_in_split_order.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "# Save invalid indices for reference\n",
    "np.save(REMOVED_IDX_FILE, np.array(invalid_indices_all, dtype=np.int64))\n",
    "\n",
    "print(f\"\\n‚úÖ Saved filtered CSV with {len(df_filtered_in_split_order)} rows (out of {len(df_concat)}).\")\n",
    "print(f\"üóëÔ∏è Removed {len(invalid_indices_all)} invalid patches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e6fa2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping of old patches with csv rows\n",
    "\n",
    "old_csv = pd.read_csv(\"/home/ubuntu/mucilage_pipeline/mucilage-detection/csv/patches_final.csv\")\n",
    "\n",
    "# Recreate the old splits the same way\n",
    "df_train_old, df_val_old, df_test_old = split_data(\"/home/ubuntu/mucilage_pipeline/mucilage-detection/csv/patches_final.csv\")\n",
    "\n",
    "# Combine them in the same order as before (train ‚Üí val ‚Üí test)\n",
    "df_all_old = pd.concat([df_train_old, df_val_old, df_test_old], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Load old caches (the ones used to make rgb_patches)\n",
    "train_data = np.load(\"/home/ubuntu/mucilage_pipeline/mucilage-detection/saved_npy/train_cache_old.npz\")\n",
    "val_data = np.load(\"/home/ubuntu/mucilage_pipeline/mucilage-detection/saved_npy/val_cache_old.npz\")\n",
    "test_data = np.load(\"/home/ubuntu/mucilage_pipeline/mucilage-detection/saved_npy/test_cache_old.npz\")\n",
    "\n",
    "# Combine labels\n",
    "y_all = np.concatenate([train_data[\"y\"], val_data[\"y\"], test_data[\"y\"]])\n",
    "\n",
    "\n",
    "# Get the indices of positive patches (mucilage)\n",
    "pos_indices = np.where(y_all == 1)[0]\n",
    "df_pos = df_all_old.iloc[pos_indices].reset_index(drop=True)\n",
    "\n",
    "# Each positive corresponds 1:1 to your patch_XXXX.png export order\n",
    "df_pos[\"rgb_filename\"] = [f\"patch_{i:04d}.png\" for i in range(len(df_pos))]\n",
    "\n",
    "df_pos.to_csv(\"/home/ubuntu/mucilage_pipeline/mucilage-detection/csv/rgb_patch_mapping.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfaabdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© 39 patches dropped due to NaNs\n"
     ]
    }
   ],
   "source": [
    "# add index column to the csvs\n",
    "\n",
    "df_orig = pd.read_csv(\"/home/ubuntu/mucilage_pipeline/mucilage-detection/csv/patches_final.csv\")\n",
    "df_filt = pd.read_csv(\"/home/ubuntu/mucilage_pipeline/mucilage-detection/csv/patches_final_filtered.csv\")\n",
    "OUTPUT_FILTERED_CSV = \"/home/ubuntu/mucilage_pipeline/mucilage-detection/csv/patches_final_filtered_split.csv\"\n",
    "\n",
    "# Recreate original splits (if not already stored)\n",
    "df_train_orig, df_val_orig, df_test_orig = split_data(\"/home/ubuntu/mucilage_pipeline/mucilage-detection/csv/patches_final.csv\")\n",
    "for df in [df_train_orig, df_val_orig, df_test_orig, df_filt]:\n",
    "    df[\"patch_id\"] = df[\"zarr_path\"].astype(str) + \"_\" + df[\"x\"].astype(str) + \"_\" + df[\"y\"].astype(str)\n",
    "\n",
    "# Add split info\n",
    "df_train_orig[\"split\"] = \"train\"\n",
    "df_val_orig[\"split\"] = \"val\"\n",
    "df_test_orig[\"split\"] = \"test\"\n",
    "df_orig_splits = pd.concat([df_train_orig, df_val_orig, df_test_orig], axis=0).reset_index(drop=True)\n",
    "\n",
    "orig_keys = list(zip(df_orig_splits[\"zarr_path\"], df_orig_splits[\"x\"].astype(float), df_orig_splits[\"y\"].astype(float)))\n",
    "filt_keys = set(zip(df_filt[\"zarr_path\"], df_filt[\"x\"].astype(float), df_filt[\"y\"].astype(float)))\n",
    "removed_indices = [i for i, key in enumerate(orig_keys) if key not in filt_keys]\n",
    "print(f\"üß© {len(removed_indices)} patches dropped due to NaNs\")\n",
    "\n",
    "df_filtered_split = df_filt.merge(\n",
    "    df_orig_splits[[\"patch_id\", \"split\"]],\n",
    "    on=\"patch_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df_filtered_split.drop(\"patch_id\", axis=1, inplace=True)\n",
    "df_filtered_split.to_csv(OUTPUT_FILTERED_CSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea14a8f",
   "metadata": {},
   "source": [
    "Save as numpy the mucilage patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab57bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 154 positive patches out of 955 total.\n"
     ]
    }
   ],
   "source": [
    "# Path to your cached .npz file\n",
    "cache_file = \"/home/ubuntu/mucilage_pipeline/mucilage-detection/saved_npy/train_cache.npz\"  # or val.npz / test.npz\n",
    "out_file = \"/home/ubuntu/mucilage_pipeline/mucilage-detection/saved_npy/train_positive_new.npz\"\n",
    "\n",
    "# Load the cached dataset\n",
    "data = np.load(cache_file)\n",
    "X, y = data[\"X\"], data[\"y\"]  # X.shape = (N,H,W,C), y.shape = (N,)\n",
    "\n",
    "# Select only patches with label 1 (mucilage)\n",
    "mask = y == 1\n",
    "X_pos = X[mask]\n",
    "y_pos = y[mask]  # will be all 1, optional\n",
    "\n",
    "print(f\"Selected {len(X_pos)} positive patches out of {len(y)} total.\")\n",
    "\n",
    "# Save to a new npz file\n",
    "# np.savez_compressed(out_file, X=X_pos, y=y_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b6785",
   "metadata": {},
   "source": [
    "Convert and save as RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c46e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203, 256, 256, 11)\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/ubuntu/mucilage_pipeline/mucilage-detection/saved_npy/train_positive.npz\"\n",
    "arr = np.load(path)\n",
    "X = arr['X']  # (N, H, W, C)\n",
    "output_dir = \"/home/ubuntu/mucilage_pipeline/mucilage-detection/rgb_patches\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i, patch in enumerate(X):\n",
    "    rgb = patch[:, :, [3, 2, 1]]  # RGB bands\n",
    "    # Percentile normalization\n",
    "    p2, p98 = np.nanpercentile(rgb, (2, 98))\n",
    "    rgb = np.clip((rgb - p2) / (p98 - p2 + 1e-6), 0, 1)\n",
    "    rgb = (rgb * 255).astype(np.uint8)\n",
    "    # Save each patch as PNG\n",
    "    filename = os.path.join(output_dir, f\"patch_{i:04d}.png\")\n",
    "    cv2.imwrite(filename, cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d57d9f4",
   "metadata": {},
   "source": [
    "Convert annotations from Roboflow to binary masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your dataset\n",
    "base_dir = \"/home/ubuntu/mucilage_pipeline/mucilage-detection/roboflow_dataset\"\n",
    "splits = [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "for split in splits:\n",
    "    img_dir = os.path.join(base_dir, split)\n",
    "    ann_path = os.path.join(img_dir, \"_annotations.coco.json\")\n",
    "    mask_dir = os.path.join(base_dir, f\"masks_{split}\")\n",
    "    os.makedirs(mask_dir, exist_ok=True)\n",
    "\n",
    "    coco = COCO(ann_path)\n",
    "\n",
    "    for img_id in coco.getImgIds():\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_name = img_info[\"file_name\"]\n",
    "        h, w = img_info[\"height\"], img_info[\"width\"]\n",
    "\n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        for ann in anns:\n",
    "            # Convert polygon or RLE to binary mask\n",
    "            m = coco.annToMask(ann)\n",
    "            mask = np.maximum(mask, m * 255)\n",
    "\n",
    "        cv2.imwrite(os.path.join(mask_dir, img_name), mask)\n",
    "\n",
    "    print(f\"‚úÖ Masks for {split} saved to {mask_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "716eaef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ train masks generated: (955, 256, 256)\n",
      "‚úÖ val masks generated: (203, 256, 256)\n",
      "‚úÖ test masks generated: (210, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "# Roboflow dataset with all images togethes (not splitted)\n",
    "\n",
    "PATCH_SIZE = 256\n",
    "CSV_PATH = \"/home/ubuntu/mucilage_pipeline/mucilage-detection/csv/patches_final_filtered_split.csv\"\n",
    "CACHE_DIR = \"/home/ubuntu/mucilage_pipeline/mucilage-detection/saved_npy\"\n",
    "COCO_PATH = \"/home/ubuntu/mucilage_pipeline/mucilage-detection/roboflow_dataset/train/_annotations.coco.json\"\n",
    "MASKS_OUT = \"/home/ubuntu/mucilage_pipeline/mucilage-detection/roboflow_dataset/saved_masks\"\n",
    "os.makedirs(MASKS_OUT, exist_ok=True)\n",
    "\n",
    "# Load CSV & Create Splits\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Load COCO annotations\n",
    "with open(COCO_PATH, 'r') as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "anns_by_image = {}\n",
    "for ann in coco['annotations']:\n",
    "    anns_by_image.setdefault(ann['image_id'], []).append(ann)\n",
    "\n",
    "img_map = {img['id']: img['file_name'] for img in coco['images']}\n",
    "\n",
    "# Map RGB patch index (cumulative) -> annotations\n",
    "rgb_ann_map = {}\n",
    "for img_id, filename in img_map.items():\n",
    "    base = os.path.basename(filename)\n",
    "    idx_str = base.split('_')[1]  # '0000', '0001', ...\n",
    "    rgb_ann_map[idx_str] = anns_by_image.get(img_id, [])\n",
    "\n",
    "def ann_to_mask(annotations, h, w):\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    for ann in annotations:\n",
    "        seg = ann['segmentation']\n",
    "        if isinstance(seg, list):\n",
    "            for poly in seg:\n",
    "                poly = np.array(poly).reshape((-1, 2))\n",
    "                img = Image.new('L', (w, h), 0)\n",
    "                ImageDraw.Draw(img).polygon(poly.flatten().tolist(), outline=1, fill=1)\n",
    "                mask += np.array(img, dtype=np.uint8)\n",
    "        else:\n",
    "            m = maskUtils.decode(seg)\n",
    "            mask = np.maximum(mask, m)\n",
    "    return np.clip(mask, 0, 1)\n",
    "\n",
    "# Process all splits\n",
    "global_pos_counter = 0\n",
    "for split_name in [\"train\", \"val\", \"test\"]:\n",
    "    cache_path = os.path.join(CACHE_DIR, f\"{split_name}_cache.npz\")\n",
    "    data = np.load(cache_path)\n",
    "    X, y = data[\"X\"], data[\"y\"]\n",
    "\n",
    "    total_masks = []\n",
    "    for label in y:\n",
    "        if label == 0:\n",
    "            mask = np.zeros((PATCH_SIZE, PATCH_SIZE), dtype=np.uint8)\n",
    "        else:\n",
    "            idx_str = f\"{global_pos_counter:04d}\"  # cumulative index\n",
    "            anns = rgb_ann_map.get(idx_str, [])\n",
    "            if len(anns) == 0:\n",
    "                mask = np.zeros((PATCH_SIZE, PATCH_SIZE), dtype=np.uint8)\n",
    "                print(f\"‚ö†Ô∏è Missing annotation for positive patch {idx_str}, using zeros.\")\n",
    "            else:\n",
    "                mask = ann_to_mask(anns, PATCH_SIZE, PATCH_SIZE)\n",
    "            global_pos_counter += 1\n",
    "        total_masks.append(mask)\n",
    "\n",
    "    total_masks = np.stack(total_masks)\n",
    "    np.savez_compressed(os.path.join(MASKS_OUT, f\"{split_name}_masks.npz\"), masks=total_masks)\n",
    "    print(f\"‚úÖ {split_name} masks generated: {total_masks.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ccd7ed",
   "metadata": {},
   "source": [
    "Reconstruct original order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "412ca0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(labels_file, test_size=0.3, val_size=0.5, seed=42):\n",
    "    df = pd.read_csv(labels_file)\n",
    "\n",
    "    # first split train vs test\n",
    "    df_train, df_tmp = train_test_split(\n",
    "        df, test_size=test_size, stratify=df[\"label\"], random_state=seed\n",
    "    )\n",
    "    # then split train vs val\n",
    "    df_test, df_val = train_test_split(\n",
    "        df_tmp, test_size=val_size, stratify=df_tmp[\"label\"], random_state=seed\n",
    "    )\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "df = \"/home/ubuntu/mucilage_pipeline/mucilage-detection/csv/patches_final.csv\"\n",
    "df_train, df_val, df_test = split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6860803f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing TRAIN ===\n",
      "Found 222 refined masks for train.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 154/154 [00:00<00:00, 4052.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved refined masks to /home/ubuntu/mucilage_pipeline/mucilage-detection/saved_npy/train_masks_refined.npz\n",
      "\n",
      "=== Processing VAL ===\n",
      "Found 222 refined masks for val.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:00<00:00, 6921.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved refined masks to /home/ubuntu/mucilage_pipeline/mucilage-detection/saved_npy/val_masks_refined.npz\n",
      "\n",
      "=== Processing TEST ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 222 refined masks for test.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:00<00:00, 3230.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved refined masks to /home/ubuntu/mucilage_pipeline/mucilage-detection/saved_npy/test_masks_refined.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/home/ubuntu/mucilage_pipeline/mucilage-detection\"\n",
    "splits = {\n",
    "    \"train\": df_train,\n",
    "    \"val\": df_val,\n",
    "    \"test\": df_test\n",
    "}\n",
    "\n",
    "for split, df in splits.items():\n",
    "    print(f\"\\n=== Processing {split.upper()} ===\")\n",
    "\n",
    "    # Load original npz cache\n",
    "    cache_file = os.path.join(base_dir, f\"saved_npy/{split}_cache.npz\")\n",
    "    data = np.load(cache_file)\n",
    "    X, y = data[\"X\"], data[\"y\"]\n",
    "\n",
    "    # Identify positive/negative indices\n",
    "    # Indices\n",
    "    pos_indices = np.where(y == 1)[0]\n",
    "    neg_indices = np.where(y == 0)[0]\n",
    "\n",
    "    # Initialize empty masks (same number of patches as X)\n",
    "    H, W = 256, 256\n",
    "    M = np.zeros((len(X), H, W), dtype=np.uint8)\n",
    "\n",
    "    # Refined masks directory (from Roboflow export)\n",
    "    mask_dir = os.path.join(base_dir, f\"roboflow_dataset/masks\")\n",
    "\n",
    "    # Build prefix-based lookup dictionary for Roboflow masks\n",
    "    mask_lookup = {}\n",
    "    for fname in os.listdir(mask_dir):\n",
    "        if fname.startswith(\"patch_\"):\n",
    "            prefix = fname.split(\"_png\")[0]  # e.g. \"patch_0000\"\n",
    "            mask_lookup[prefix] = os.path.join(mask_dir, fname)\n",
    "\n",
    "    print(f\"Found {len(mask_lookup)} refined masks for {split}.\")\n",
    "\n",
    "    # Fill masks for positive patches\n",
    "    for i, idx in enumerate(tqdm(pos_indices)):\n",
    "        prefix = f\"patch_{i:04d}\"  # matches your exported patch names\n",
    "        mask_path = mask_lookup.get(prefix, None)\n",
    "        if mask_path:\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if mask is not None:\n",
    "                M[idx] = (mask > 127).astype(np.uint8)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Could not read {mask_path}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Missing mask for {prefix} ({split})\")\n",
    "\n",
    "    # Save mask array aligned with original X\n",
    "    out_path = os.path.join(base_dir, f\"saved_npy/{split}_masks_refined.npz\")\n",
    "    np.savez_compressed(out_path, M=M)\n",
    "    print(f\"‚úÖ Saved refined masks to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82fabe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eopf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
